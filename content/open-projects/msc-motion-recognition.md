+++
title = "MSc graduation project: Machine Learning for Human Motion Recognition"
date = "2017-08-24T15:50:58+02:00"
draft = true

description = "In this project, you are challenged to develop novel machine learning technology for recognizing human motions."
external_link = ""
vacancy_id = "motion-recognition"
+++


## Context

{{< figure src="/img/biaslab-logo.png" title="BIASlab logo" class="right-inline" width="200px" >}}

**BIASlab** (Fig.1, <http://biaslab.github.io>, FLUX-7.060) is
a subgroup of the Signal Processing Systems (SPS) that aims to develop
**Intelligent Autonomous Agents** (IA). These agents interact with their
environment through their sensors and actuators in order to learn
purposeful behavior, e.g., to navigate, play soccer or they may learn to
decode speech signals under bad acoustic conditions. Our research
projects are inspired by the latest insights from machine learning,
computational neuroscience and signal processing.

## Project Description

{{< figure src="/img/proposals/gesture-model.png" title="Motion model" class="left-inline" width="300px" >}}

In this project, you are challenged to design **a machine learning agent that learns to recognize specific motions** through on-the-spot interaction with a (human) end user. Motion recognition is a broad field with applications such as *gesture* recognition (e.g., for recognizing sign language or remote device control), *activity* recognition (e.g., for fall detection) and *motion analysis* (e.g., for rehabilitation). You are challenged to develop autonomous learning agents that are informed by body-worn motion sensors, e.g., by accelerometers in smart [ear buds](http://www.jabra.com/cp/us/pressreleasesarchive/2016/press-release-06-january-2016) or [wristband](https://www.empatica.com/e4-wristband) devices. In this project, we will primarily focus on **one-shot learning** of motions: how can we learn specific and complicated motions by an individual user from only a few examples (ideally only one) by that user? This project will get you involved with the latest **artificial
intelligence** methods, since the agent will need to (1) be smart about selecting the most informative motion examples, and (2) learn as much as possible (but not more) from each
motion example. These are typical qualities of human learning and this project will also give you an opportunity to learn
about how biological brains solve real-time design issues. This project builds upon our previous work on [gesture recognition](/project/a-probabilistic-approach-to-in-situ-trainable-gesture-recognition).

## Timing

Start date: the project is available from **September 2017** (or any time thereafter).

Duration: 9 months (fte).

## Financial Support

{{< figure src="/img/proposals/GN-logo.png" class="left-inline" width="100px" >}}

The audio solutions company GN (<http://www.gn.com/>) may financially support strong candidates (qualifications to be discussed with prof. de Vries) by a **GN scholarship**.

## Contact

For more information about this project, please contact Prof. Bert de
Vries (<bert.de.vries@tue.nl>). Also, feel free to make an appointment
to discuss alternative projects with intelligent autonomous agents that
you might be interested in.
